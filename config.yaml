# GPU Kernel Lab Configuration
# ================================

# Hardware configuration
hardware:
  # Options: "native" (Blackwell sm_100) or "compile_only" (non-Blackwell, e.g. RTX 3060)
  # In compile_only mode, kernels compile for sm_100 but skip execution
  hardware_mode: native
  
  # Target SM architecture for compilation
  target_sm: 100
  
  # CUDA tools paths
  ptxas_path: /usr/local/cuda/bin/ptxas
  cuobjdump_path: /usr/local/cuda/bin/cuobjdump

# Output directory for artifacts (PTX, CUBIN, SASS, reports)
output_dir: outputs

# Kernel configurations
kernels:
  add:
    enabled: true
    # Available backends: cublas, triton, cutile, pytorch
    backends:
      - triton
     # - cutile
      - pytorch
     # - cublas
    # Tile/block size for kernel execution
    tile_size: 16
  
  matmul:
    enabled: true
    # Available backends: triton, cutile, pytorch
    backends:
      - triton
    #  - cutile
      - pytorch

# Test configurations for vector add
tests:
  enabled: true
  # Filter tests by name pattern (like pytest -k). Set to null or "" to run all tests.
  # Examples: "random", "float32", "compile", "correctness"
  #name_filter: compile
  #name_filter: random
  # Which backends to test
  backends:
    - triton
    #- cutile
    - pytorch
    #- cublas
  # Input sizes for correctness tests
  sizes:
    - 1024
    - 4096
    - 16384
    - 65536
  # Data types to test
  dtypes:
    - float32
    - float16

# Test configurations for matrix multiplication
tests_matmul:
  enabled: true
  # Filter tests by name pattern
  name_filter: null
  # Which backends to test
  backends:
    - triton
  #  - cutile
    - pytorch
  # Matrix dimensions - all combinations of M × N × K will be tested
  M: [64, 128]
  N: [128, 256]
  K: [256, 512]
  # Data types to test
  dtypes:
    - float32
    - float16

# Benchmark configurations for vector add
benchmarks:
  enabled: true
  # Which backends to benchmark
  backends:
    - triton
    - cutile
    - pytorch
    #- cublas
  # Input sizes for benchmarks
  sizes:
    - 65536
    - 262144
    - 1048576
    - 4194304
  # Warmup iterations before timing
  #warmup_iterations: 10
  warmup_iterations: 5
  # Number of benchmark iterations
  #benchmark_iterations: 100
  benchmark_iterations: 20
  # Metrics to compute
  metrics:
    - latency_us
    - tflops
    - memory_bandwidth_gbps

# Benchmark configurations for matrix multiplication
benchmarks_matmul:
  enabled: true
  # Which backends to benchmark
  backends:
    - triton
    #- cutile
    - pytorch
  # Matrix dimensions - all combinations of M × N × K will be benchmarked
  M: [1024, 2048]
  N: [1024, 2048]
  K: [1024, 2048]
  # Warmup iterations before timing
  warmup_iterations: 5
  # Number of benchmark iterations
  benchmark_iterations: 20
  # Metrics to compute
  metrics:
    - latency_us
    - tflops
    - memory_bandwidth_gbps

# SASS analysis configuration
sass_analysis:
  enabled: true
  # Pairs of backends to compare SASS
  compare_pairs:
    - [triton, cutile]
  # Save PTX, CUBIN, SASS artifacts
  save_artifacts: true

