# Triton vs CuTile: The Blocking Synchronization Problem

## Proving TileIR's Advantages on Modern GPUs (Hopper/Blackwell)

---

## ðŸ“š Foundation: The Paper's Core Insight

From **"Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs"**

### Section 3.2: Code Generation Challenges

The paper identifies **4 fundamental challenges** that plague traditional thread-based compilers (like those generating PTX):

---

### Challenge 1: Cooperative Warp Execution

**Problem:** Modern Tensor Core operations (e.g., `tcgen05.mma` on SM100) require **multi-warp cooperation**. Traditional compilers model single-threaded SIMT execution.

**Thread-based (PTX):**
```
Each thread has no knowledge of other warps
â†’ Cannot express "warp 0 and warp 1 collaborate on this tile"
```

**Tile-based (TileIR):**
```
Tile operations inherently express multi-warp semantics
â†’ Compiler can distribute work across warps naturally
```

---

### Challenge 2: Register Pressure

**Problem:** Software pipelining with modulo schedules requires storing multiple loop iterations' worth of data. Combined with large TF32/BF16 tiles, register usage can exceed 255 (hardware limit).

**Thread-based (PTX):**
```
- Static register allocation
- No visibility into tile structure
- Results in register spilling (slow!)
```

**Tile-based (TileIR):**
```
- setmaxnreg for dynamic register allocation (Blackwell feature)
- Explicit shared memory management
- Better occupancy control
```

---

### Challenge 3: Variable Latency TMA Transfers

**Problem:** TMA (Tensor Memory Accelerator) transfers have **10-100x latency variance** depending on memory access patterns, cache state, and congestion.

**Thread-based (PTX):**
```
- Static synchronization: "wait 50 cycles"
- Wastes time if data arrives early
- Stalls if data arrives late
```

**Tile-based (TileIR):**
```
- Async barriers with dynamic wait
- cp.async.bulk with mbarrier: "wake me when ready"
- Better utilization of variable-latency operations
```

---

### Challenge 4: Blocking Synchronization â¬…ï¸ **OUR EXPERIMENT**

**Problem:** When GEMM (Tensor Core) and EXP (Special Function Unit) run on the **same warp**, the warp must wait for GEMM before starting EXP, even though they use **different functional units**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Single Warp (Triton):        Multi-Warp (CuTile potential):    â”‚
â”‚                                                                 â”‚
â”‚  â”œâ”€ GEMM (Tensor Core)        â”œâ”€ Warp 0: GEMM (Tensor Core)     â”‚
â”‚  â”œâ”€ WAIT â—€â”€â”€ BLOCKS!          â”‚                                 â”‚
â”‚  â””â”€ EXP (SFU)                 â”œâ”€ Warp 1: EXP (SFU) â† overlap!   â”‚
â”‚                               â””â”€ Sync only when needed          â”‚
â”‚                                                                 â”‚
â”‚  Timeline:                    Timeline:                         â”‚
â”‚  |GEMM---|WAIT|EXP---|        |GEMM---|                         â”‚
â”‚                               |----EXP---|                     â”‚
â”‚                               â†‘ Overlapped execution!           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**This is what we test in our experiment!**

---

## ðŸ§ª Our Experiment: GEMM + EXP Kernel Fusion

### What We Compute

```python
C = exp(A @ B)
```

A simple fused kernel with:
1. **Matrix multiplication** (GEMM) â†’ Uses Tensor Cores
2. **Exponential function** (EXP) â†’ Uses Special Function Unit (MUFU)

**Key question:** Does the compiler insert unnecessary blocking synchronization between these independent phases?

---

## ðŸ’» The Code: Triton vs CuTile

### Triton Implementation

```python
@triton.jit
def _gemm_exp_kernel(
    A_ptr, B_ptr, C_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    # Program ID
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # Block offsets
    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    # Pointers
    a_ptrs = A_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = B_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)
    
    # Accumulator
    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    # GEMM loop
    for k in range(0, tl.cdiv(K, BLOCK_K)):
        a = tl.load(a_ptrs, mask=..., other=0.0)
        b = tl.load(b_ptrs, mask=..., other=0.0)
        
        # Tensor Core GEMM (async)
        accumulator = tl.dot(a, b, accumulator)
        
        a_ptrs += BLOCK_K * stride_ak
        b_ptrs += BLOCK_K * stride_bk
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # HERE IS THE BLOCKING SYNC ISSUE!
    # 
    # The accumulator must be materialized before EXP.
    # Same warp â†’ wait BLOCKS everything!
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Apply EXP (requires GEMM result first!)
    c = tl.exp(accumulator)  # â† Forced to wait!
    
    # Store result
    tl.store(c_ptrs, c, mask=c_mask)
```

**Lines of code:** ~90 lines (full implementation with setup)

---

### CuTile Implementation

```python
@ct.kernel(num_ctas=ct.ByTarget(sm_100=2))  # Multi-CTA hint!
def gemm_exp_kernel(
    A, B, C,
    tm: ct.Constant[int],
    tn: ct.Constant[int],
    tk: ct.Constant[int]
):
    M = A.shape[0]
    N = B.shape[1]
    
    bidx = ct.bid(0) // ct.cdiv(N, tn)
    bidy = ct.bid(0) % ct.cdiv(N, tn)
    
    num_tiles_k = ct.num_tiles(A, axis=1, shape=(tm, tk))
    
    # Accumulator in fp32
    accumulator = ct.full((tm, tn), 0, dtype=ct.float32)
    
    # GEMM loop - Tensor Core MMA
    for k in range(num_tiles_k):
        a = ct.load(A, index=(bidx, k), shape=(tm, tk), padding_mode=ct.PaddingMode.ZERO)
        b = ct.load(B, index=(k, bidy), shape=(tk, tn), padding_mode=ct.PaddingMode.ZERO)
        accumulator = ct.mma(a, b, accumulator)  # Tile-level MMA!
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CuTile's approach:
    # 
    # TileIR represents tile-level operations that can be
    # distributed across warps by the backend compiler.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Apply EXP element-wise
    result = ct.exp(accumulator)  # â† Can schedule differently!
    
    # Store
    ct.store(C, index=(bidx, bidy), tile=result)
```

**Lines of code:** ~40 lines (much more concise!)

---

## ðŸ“Š Code Comparison

| Aspect | Triton | CuTile |
|--------|--------|--------|
| **Lines of Code** | ~90 | ~40 |
| **Abstraction Level** | Thread offsets, masks, strides | Tile indices, shapes |
| **Memory Management** | Manual pointer arithmetic | Automatic padding |
| **Tensor Core Access** | `tl.dot()` (implicit) | `ct.mma()` (explicit) |
| **Multi-CTA Support** | Not directly expressible | `num_ctas=2` hint |

---

## âš ï¸ Important Theoretical Point

### Both Generate PTX!

```
Triton Kernel  â”€â”€compileâ”€â”€â–¶  PTX (.version 9.1)  â”€â”€ptxasâ”€â”€â–¶  SASS/CUBIN
CuTile Kernel  â”€â”€compileâ”€â”€â–¶  PTX (.version 9.1)  â”€â”€ptxasâ”€â”€â–¶  SASS/CUBIN
```

**Theoretically**, we can't say "CuTile is fundamentally better" because:
- Both ultimately generate PTX (same low-level IR)
- Final SASS is produced by NVIDIA's `ptxas` compiler
- If CuTile generated SASS directly, we could make stronger claims

**However**, the quality of the PTX differs significantly! This affects what `ptxas` can optimize.

---

## ðŸ“ˆ Benchmark Results (Modal B200 GPU)

### Experiment: GEMM+EXP on Blackwell B200

**Configuration:**
- PTX Version: 9.1 (CUDA 13.1)
- warmup=5, runs=20

### Square Matrices

| Matrix Size | Triton | CuTile | Speedup |
|------------|--------|--------|---------|
| 512 Ã— 512 | 8.71 TFLOPS | 17.21 TFLOPS | **1.98Ã—** |
| 1024 Ã— 1024 | 42.90 TFLOPS | 54.99 TFLOPS | **1.28Ã—** |
| 2048 Ã— 2048 | 62.77 TFLOPS | 92.56 TFLOPS | **1.47Ã—** |

### Transformer-like Shapes (M=batch, K=hidden, N=projection)

| Shape (MÃ—KÃ—N) | Triton | CuTile | Speedup |
|---------------|--------|--------|---------|
| 32 Ã— 8192 Ã— 1024 | 3.22 TFLOPS | 11.84 TFLOPS | **3.67Ã—** |
| 8192 Ã— 8192 Ã— 1024 | 59.48 TFLOPS | 117.50 TFLOPS | **1.98Ã—** |
| 8192 Ã— 8192 Ã— 128 | 52.18 TFLOPS | 99.33 TFLOPS | **1.90Ã—** |

### Summary

- **CuTile wins on all tested configurations**
- **Speedup range: 1.28Ã— to 3.67Ã—**
- Highest speedup on small-batch cases (3.67Ã— at M=32)

---

## ðŸ”¬ PTX Analysis: What's Different?

### PTX Version Verification

Both frameworks generate **identical PTX version**:
```
Triton PTX: .version 9.1  âœ“
CuTile PTX: .version 9.1  âœ“
Target: sm_100a (Blackwell)
```

### Standard PTX Operations

| Metric | Triton | CuTile | Diff |
|--------|--------|--------|------|
| MMA (mma/wmma/wgmma) | 8 | 4 | **-4** |
| EXP (ex2) | 32 | 0 | -32 âš ï¸ |
| Basic barriers (bar.sync) | **43** | **1** | **-42** |
| Fence instructions | 0 | 4 | +4 |
| Async copy (cp.async) | 139 | 8 | **-131** |

âš ï¸ Note: CuTile uses a different EXP implementation (fp16 conversion path)

---

### Blackwell SM100 Operations

| Metric | Triton | CuTile | Diff |
|--------|--------|--------|------|
| tcgen05.mma (Tensor Core Gen5) | 8 | 4 | -4 |
| tcgen05.ld (TC loads) | 1 | **16** | +15 |
| tcgen05.st (TC stores) | 1 | **16** | +15 |
| tcgen05.wait | 2 | 2 | 0 |
| tcgen05.commit | 2 | 2 | 0 |
| mbarrier (memory barriers) | 6 | 23 | +17 |
| barrier.cluster | 0 | **8** | +8 |
| cp.async.bulk (TMA) | 0 | **8** | +8 |
| setmaxnreg (dynamic regs) | 0 | **6** | +6 |

---

## ðŸŽ¯ Key PTX Insights

### 1. âœ“ CuTile Uses Bulk TMA

```ptx
// CuTile: TMA bulk copy (efficient DMA)
cp.async.bulk.tensor.2d.global.shared::cta.tile.bulk_group [%rd8, {%r172, %r175}], [%r177];
```

```ptx
// Triton: Many individual cp.async (less efficient)
cp.async.ca.shared.global [ %r124 + 0 ], [ %rd78 + 0 ], 0x4, %r125;
```

**CuTile: 8 bulk TMA vs Triton: 0 (uses 139 regular cp.async)**

---

### 2. âœ“ CuTile Has 6.8Ã— Fewer Sync Operations

| Backend | bar.sync | Total Sync-like Ops |
|---------|----------|---------------------|
| Triton | 43 | **48** |
| CuTile | 1 | **7** |

**Fewer barriers = less thread blocking = better utilization!**

---

### 3. âœ“ CuTile Uses Dynamic Register Allocation

```ptx
// CuTile: Blackwell-specific dynamic register control
setmaxnreg.inc.sync.aligned.u32 232;  // Increase to 232 regs
setmaxnreg.dec.sync.aligned.u32 32;   // Decrease to 32 regs
```

**6 setmaxnreg instructions** allow optimal register usage per kernel phase.

Triton: **0 setmaxnreg** (static allocation only)

---

### 4. âœ“ CuTile Leverages Cluster Barriers

```ptx
// CuTile: SM90+ cluster-level synchronization
barrier.cluster.arrive.relaxed.aligned;
barrier.cluster.wait.aligned;
```

**8 barrier.cluster** instructions for efficient multi-CTA sync.

Triton: **0 barrier.cluster** (CTA-level only)

---

### 5. âš ï¸ Triton Blocking Pattern Confirmed

In Triton's PTX, we found:

```
âš ï¸ Found 5 sync ops between last MMA (L1581) and first EXP (L1799)
```

This confirms the paper's observation: **EXP is blocked by GEMM.WAIT** in the single-warp execution model.

---

## ðŸ“Š SASS-Level Evidence

### Synchronization Instructions

| Metric | Triton | CuTile |
|--------|--------|--------|
| GEMM ops (MMA) | 0* | 4 |
| EXP ops | 32 | 0* |
| **Sync ops** | **48** | **7** |

*Different instruction choices for same operation

**CuTile: 6.8Ã— fewer sync operations!**

---

## ðŸ† Conclusion

### Why CuTile Wins

1. **Better Blackwell Utilization**
   - TMA bulk copies (cp.async.bulk)
   - Cluster barriers (barrier.cluster)  
   - Dynamic register allocation (setmaxnreg)

2. **Fewer Blocking Synchronizations**
   - 6.8Ã— fewer sync barriers
   - Less wasted time waiting

3. **Tile-Level Abstraction**
   - Simpler code (40 lines vs 90 lines)
   - Multi-CTA hints (`num_ctas=2`)
   - Compiler can optimize tile placement

---

### Performance Summary

| Metric | Value |
|--------|-------|
| **Speedup Range** | 1.28Ã— - 3.67Ã— |
| **Sync Reduction** | 6.8Ã— fewer barriers |
| **Code Simplicity** | 2Ã— fewer lines |
| **Peak Performance** | 117 TFLOPS (CuTile) vs 59 TFLOPS (Triton) |

---

### The Theoretical Caveat

While CuTile achieves significant speedups through better PTX generation:

> **Both Triton and CuTile generate PTX**, so we cannot claim fundamental architectural superiority.
>
> If CuTile generated SASS directly, bypassing `ptxas`, we could make stronger claims about its optimization capabilities.

**What we CAN claim:** CuTile's tile-level abstractions produce **better-quality PTX** that results in more efficient SASS on Blackwell GPUs.

---

## ðŸ”— References

1. **Paper:** "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs" (Section 3.2)
2. **Triton:** https://triton-lang.org/
3. **CuTile:** https://pypi.org/project/cuda-tile/
4. **Experiment Code:** `experiments/04_blocking_sync/`

---

*Generated from triton_cutile benchmark project*

*Hardware: Modal B200 GPU | CUDA 13.1 | PTX 9.1*
